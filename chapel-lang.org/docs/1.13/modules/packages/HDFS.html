

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>HDFS &mdash; Chapel Documentation 1.13</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../_static/style.css" type="text/css" />
  

  
    <link rel="top" title="Chapel Documentation 1.13" href="../../index.html"/>
        <link rel="up" title="Package Modules" href="../packages.html"/>
        <link rel="next" title="HDFSiterator" href="HDFSiterator.html"/>
        <link rel="prev" title="FFTW_MT" href="FFTW_MT.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> Chapel Documentation
          

          
          </a>

          
            
            
          

          
<?php   // Variables given by sphinx 
   $chplTitle = "1.13";   $pagename = "./modules/packages/HDFS";   include "../../versionButton.php";   ?>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <p class="caption"><span class="caption-text">Compiling and Running Chapel</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../usingchapel/QUICKSTART.html">Quickstart Instructions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../usingchapel/index.html">Using Chapel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../platforms/index.html">Platform-Specific Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../technotes/index.html">Technical Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tools/index.html">Tools</a></li>
</ul>
<p class="caption"><span class="caption-text">Writing Chapel Programs</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../language/reference.html">Quick Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../language/spec.html">Language Specification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../builtins.html">Built-in Types and Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules.html">Standard Modules</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../packages.html">Package Modules</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Curl.html">Curl</a></li>
<li class="toctree-l2"><a class="reference internal" href="FFTW.html">FFTW</a></li>
<li class="toctree-l2"><a class="reference internal" href="FFTW_MT.html">FFTW_MT</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">HDFS</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dependencies">Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="#enabling-hdfs-support">Enabling HDFS Support</a></li>
<li class="toctree-l3"><a class="reference internal" href="#using-hdfs-support-in-chapel">Using HDFS Support in Chapel</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-an-hdfs-filesystem-with-open-url-hdfs">Using an HDFS filesystem with open(url=&quot;hdfs://...&quot;)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#explicitly-using-replicated-hdfs-connections-and-files">Explicitly Using Replicated HDFS Connections and Files</a></li>
<li class="toctree-l4"><a class="reference internal" href="#explicitly-using-local-hdfs-connections-and-files">Explicitly Using Local HDFS Connections and Files</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#setting-up-hadoop">Setting up Hadoop</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hdfs-support-types-and-functions">HDFS Support Types and Functions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="HDFSiterator.html">HDFSiterator</a></li>
<li class="toctree-l2"><a class="reference internal" href="LAPACK.html">LAPACK</a></li>
<li class="toctree-l2"><a class="reference internal" href="Norm.html">Norm</a></li>
<li class="toctree-l2"><a class="reference internal" href="RecordParser.html">RecordParser</a></li>
<li class="toctree-l2"><a class="reference internal" href="Search.html">Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="Sort.html">Sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="VisualDebug.html">VisualDebug</a></li>
<li class="toctree-l2"><a class="reference internal" href="../packages.html#index">Index</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../layoutdist.html">Standard Layouts and Distributions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../users-guide/index.html">Chapel Users Guide (WIP)</a></li>
</ul>
<p class="caption"><span class="caption-text">Language History</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../language/evolution.html">Chapel Evolution</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">Chapel Documentation 1.13</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          





<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../packages.html">Package Modules</a> &raquo;</li>
      
    <li>HDFS</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/modules/packages/HDFS.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <span class="target" id="module-HDFS"></span><div class="section" id="hdfs">
<h1>HDFS<a class="headerlink" href="#hdfs" title="Permalink to this headline">¶</a></h1>
<p>Support for Hadoop Distributed Filesystem</p>
<p>This module implements support for the
<a class="reference external" href="http://hadoop.apache.org/">Hadoop</a>
<a class="reference external" href="http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html">Distributed Filesystem</a> (HDFS).</p>
<div class="section" id="dependencies">
<h2>Dependencies<a class="headerlink" href="#dependencies" title="Permalink to this headline">¶</a></h2>
<p>The HDFS functionality in Chapel is dependent upon both Hadoop and Java being
installed.  Your <code class="docutils literal"><span class="pre">HADOOP_INSTALL</span></code>, <code class="docutils literal"><span class="pre">JAVA_INSTALL</span></code> and <code class="docutils literal"><span class="pre">CLASSPATH</span></code>
environment variables must be set as described below in
<a class="reference internal" href="#setting-up-hadoop"><span>Setting up Hadoop</span></a>.  Without this it will not compile with HDFS, even if
the flags are set. As well, the HDFS functionality is also dependent upon the
<code class="docutils literal"><span class="pre">CHPL_AUXIO_INCLUDE</span></code> and <code class="docutils literal"><span class="pre">CHPL_AUXIO_LIBS</span></code> environment variables being set
properly. For more information on how to set these properly, see
doc/technotes/auxIO.rst in a Chapel release.</p>
</div>
<div class="section" id="enabling-hdfs-support">
<h2>Enabling HDFS Support<a class="headerlink" href="#enabling-hdfs-support" title="Permalink to this headline">¶</a></h2>
<p>Once you have ensured that Hadoop and Java are installed and have the
five variables above, defined, set the environment variable
CHPL_AUX_FILESYS to 'hdfs' to enable HDFS support:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">CHPL_AUX_FILESYS</span><span class="o">=</span>hdfs
</pre></div>
</div>
<p>Then, rebuild Chapel by executing 'make' from $CHPL_HOME.</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>make
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">If HDFS support is not enabled (which is the default), all
features described below will compile successfully but will result in
an error at runtime such as: &quot;No HDFS Support&quot;.</p>
</div>
</div>
<div class="section" id="using-hdfs-support-in-chapel">
<h2>Using HDFS Support in Chapel<a class="headerlink" href="#using-hdfs-support-in-chapel" title="Permalink to this headline">¶</a></h2>
<p>There are three ways provided to open HDFS files within Chapel.</p>
<div class="section" id="using-an-hdfs-filesystem-with-open-url-hdfs">
<h3>Using an HDFS filesystem with open(url=&quot;hdfs://...&quot;)<a class="headerlink" href="#using-an-hdfs-filesystem-with-open-url-hdfs" title="Permalink to this headline">¶</a></h3>
<div class="highlight-chapel"><div class="highlight"><pre><span></span><span class="c1">// Open a file on HDFS connecting to the default HDFS instance</span>
<span class="kd">var</span> <span class="nx">f</span> <span class="o">=</span> <span class="nx">open</span><span class="p">(</span><span class="nx">mode</span><span class="o">=</span><span class="nx">iomode</span><span class="p">.</span><span class="nx">r</span><span class="p">,</span> <span class="nx">url</span><span class="o">=</span><span class="s">&quot;hdfs://host:port/path&quot;</span><span class="p">);</span>

<span class="c1">// Open up a reader and read from the file</span>
<span class="kd">var</span> <span class="nx">reader</span> <span class="o">=</span> <span class="nx">f</span><span class="p">.</span><span class="nx">reader</span><span class="p">();</span>

<span class="c1">// ...</span>

<span class="nx">reader</span><span class="p">.</span><span class="nx">close</span><span class="p">();</span>

<span class="nx">f</span><span class="p">.</span><span class="nx">close</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="section" id="explicitly-using-replicated-hdfs-connections-and-files">
<h3>Explicitly Using Replicated HDFS Connections and Files<a class="headerlink" href="#explicitly-using-replicated-hdfs-connections-and-files" title="Permalink to this headline">¶</a></h3>
<div class="highlight-chapel"><div class="highlight"><pre><span></span><span class="k">use</span> <span class="nx">HDFS</span><span class="p">;</span>

<span class="c1">// Connect to HDFS via the default username (or whichever you want)</span>
<span class="c1">//</span>
<span class="kd">var</span> <span class="nx">hdfs</span> <span class="o">=</span> <span class="nx">hdfsChapelConnect</span><span class="p">(</span><span class="s">&quot;default&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>

<span class="c1">//</span>
<span class="c1">// Create a file per locale</span>
<span class="c1">//</span>
<span class="kd">var</span> <span class="nx">gfl</span>  <span class="o">=</span> <span class="nx">hdfs</span><span class="p">.</span><span class="nx">hdfsOpen</span><span class="p">(</span><span class="s">&quot;/user/johnDoe/isThisAfile.txt&quot;</span><span class="p">,</span> <span class="nx">iomode</span><span class="p">.</span><span class="nx">r</span><span class="p">);</span>

<span class="o">..</span><span class="p">.</span>
<span class="c1">//</span>
<span class="c1">// On any given locale, you can get the local file for the locale that</span>
<span class="c1">// the task is currently running on via:</span>
<span class="c1">//</span>
<span class="kd">var</span> <span class="nx">fl</span> <span class="o">=</span> <span class="nx">gfl</span><span class="p">.</span><span class="nx">getLocal</span><span class="p">();</span>

<span class="c1">// This file can be used as with a traditional file in Chapel, by</span>
<span class="c1">// creating reader channels on it.</span>

<span class="c1">// When you are done and want to close the files and disconnect from</span>
<span class="c1">// HDFS, use:</span>

<span class="nx">gfl</span><span class="p">.</span><span class="nx">hdfsClose</span><span class="p">();</span>
<span class="nx">hdfs</span><span class="p">.</span><span class="nx">hdfsChapelDisconnect</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="section" id="explicitly-using-local-hdfs-connections-and-files">
<h3>Explicitly Using Local HDFS Connections and Files<a class="headerlink" href="#explicitly-using-local-hdfs-connections-and-files" title="Permalink to this headline">¶</a></h3>
<p>The HDFS module file also supports non-replicated values across
locales. So if you only wanted to connect to HDFS and open a file on
locale 1 you could do:</p>
<div class="highlight-chapel"><div class="highlight"><pre><span></span><span class="k">on</span> <span class="nx">Locales</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="p">{</span>
  <span class="kd">var</span> <span class="nx">hdfs</span> <span class="o">=</span> <span class="nx">hdfs_chapel_connect</span><span class="p">(</span><span class="s">&quot;default&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
  <span class="kd">var</span> <span class="nx">fl</span> <span class="o">=</span> <span class="nx">hdfs</span><span class="p">.</span><span class="nx">hdfs_chapel_open</span><span class="p">(</span><span class="s">&quot;/user/johnDoe/myFile.txt&quot;</span><span class="p">,</span> <span class="nx">iomode</span><span class="p">.</span><span class="nx">cw</span><span class="p">);</span>
  <span class="o">..</span><span class="p">.</span>
  <span class="kd">var</span> <span class="nx">read</span> <span class="o">=</span> <span class="nx">fl</span><span class="p">.</span><span class="nx">reader</span><span class="p">();</span>
  <span class="o">..</span><span class="p">.</span>
  <span class="nx">fl</span><span class="p">.</span><span class="nx">close</span><span class="p">();</span>
  <span class="nx">hdfs</span><span class="p">.</span><span class="nx">hdfs_chapel_disconnect</span><span class="p">();</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The only stipulations are that you cannot open a file in both read and
write mode at the same time. (i.e iomode.r and iomode.cw are the only
modes that are supported, due to HDFS limitations).</p>
</div>
</div>
<div class="section" id="setting-up-hadoop">
<span id="id1"></span><h2>Setting up Hadoop<a class="headerlink" href="#setting-up-hadoop" title="Permalink to this headline">¶</a></h2>
<p>If you have a working installation of Hadoop already, you can skip
this section, other than to set up your CLASSPATH environment
variable.  This section is written so that people without sudo
permission can install and use HDFS.  If you do have sudo permissions,
you can usually install all of these via a package manager.</p>
<p>The general outline for these instructions is:</p>
<blockquote>
<div><ol class="arabic simple">
<li>Install and point to the jdk to provide code Chapel needs to
compile against libhdfs (<a class="reference internal" href="#setup-hadoop-1"><span>Step 1</span></a>)</li>
<li>Install Hadoop (<a class="reference internal" href="#setup-hadoop-2"><span>Step 2</span></a>)</li>
<li>Set up Hadoop on (a) the local host or (b) a cluster of hosts
(<a class="reference internal" href="#setup-hadoop-3"><span>Step 3</span></a>)</li>
<li>Start up HDFS (<a class="reference internal" href="#setup-hadoop-4"><span>Step 4</span></a>)</li>
<li>Stop HDFS when you're done (<a class="reference internal" href="#setup-hadoop-5"><span>Step 5</span></a>)</li>
<li>Set up Chapel to run in distributed mode (<a class="reference internal" href="#setup-hadoop-6"><span>Step 6</span></a>)</li>
</ol>
</div></blockquote>
<p>First reflect your directory structure and version numbers (etc) in the
<a class="reference internal" href="#setup-hadoop-bashrc"><span>sample .bashrc</span></a> and put it in your .bashrc (or
.bash_profile -- your choice) and source whichever one you put it into.</p>
<ol class="arabic simple" id="setup-hadoop-1">
<li>Make sure you have a SERVER edition of the jdk installed and
point JAVA_INSTALL to it (see the same .bashrc below)</li>
</ol>
<ol class="arabic" id="setup-hadoop-2" start="2">
<li><p class="first">Install Hadoop</p>
<ul>
<li><p class="first">Download the latest version of Hadoop and unpack it</p>
</li>
<li><p class="first">Now in the unpacked directory, open conf/hadoop-env.sh and edit:</p>
<ul>
<li><p class="first">set <code class="docutils literal"><span class="pre">JAVA_INSTALL</span></code> to be the part before <code class="docutils literal"><span class="pre">bin/</span></code>... when you do:</p>
<blockquote>
<div><div class="highlight-sh"><div class="highlight"><pre><span></span>which java
</pre></div>
</div>
</div></blockquote>
</li>
<li><p class="first">set <code class="docutils literal"><span class="pre">HADOOP_CLASSPATH=$HADOOP_HOME/&quot;&quot;*:$HADOOP_HOME/lib/&quot;&quot;*:</span></code></p>
</li>
</ul>
</li>
<li><p class="first">Now in conf/hdfs-site.xml put the replication number that you
want for the field <code class="docutils literal"><span class="pre">dfs.replication</span></code> (this will set the
replication of blocks of the files in HDFS)</p>
</li>
<li><p class="first">Now set up passwordless ssh, if you haven't yet:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>ssh-keygen -t dsa -P <span class="s1">&#39;&#39;</span> -f ~/.ssh/id_dsa
cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys
</pre></div>
</div>
</li>
</ul>
</li>
</ol>
<ol class="arabic" id="setup-hadoop-3" start="3">
<li><p class="first">Set up Hadoop</p>
<ol class="loweralpha">
<li><p class="first">For the local host - See the
<a class="reference external" href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html">Hadoop website</a>
for good documentation on how to do this.</p>
</li>
<li><p class="first">For a cluster of hosts. If you want to run Hadoop over a cluster, there
are good tutorials online. Although it is usually as easy as making
edits to the following files in <code class="docutils literal"><span class="pre">$HADOOP_HOME/conf</span></code>:</p>
<ul>
<li><p class="first">adding the name of the nodes to <code class="docutils literal"><span class="pre">slaves</span></code></p>
</li>
<li><p class="first">putting what you want to be the namenode in <code class="docutils literal"><span class="pre">masters</span></code></p>
</li>
<li><p class="first">putting the master node in <code class="docutils literal"><span class="pre">core-site.xml</span></code> and <code class="docutils literal"><span class="pre">mapred-site.xml</span></code></p>
</li>
<li><p class="first">running:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>hadoop-daemon.sh start datanode
hadoop-daemon.sh start tasktracker
</pre></div>
</div>
</li>
</ul>
<p>After this go to your datanode site and you should see a new
datanode.</p>
<p>A good online tutorial for this as well can be found here:
<a class="reference external" href="http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html">http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html</a></p>
</li>
</ol>
</li>
</ol>
<ol class="arabic" id="setup-hadoop-4" start="4">
<li><p class="first">Start HDFS</p>
<ul>
<li><p class="first">Now all we need to do is format the namenode and start things up:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>hadoop namenode -format
start-all.sh  <span class="c1"># (This will start hdfs and the tasktracker/jobtracker)</span>
</pre></div>
</div>
</li>
<li><p class="first">In general, hadoop has the same type of commands as bash,
usually in the form:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>hadoop dfs -&lt;command&gt; &lt;regular args to that command&gt;
</pre></div>
</div>
</li>
<li><p class="first">At this point, you can compile and run Chapel programs using HDFS</p>
</li>
<li><p class="first">You can check the status of Hadoop via http, for example on a local
host (e.g., for <a class="reference internal" href="#setup-hadoop-3"><span>3a above</span></a>), using:</p>
<blockquote>
<div><ul class="simple">
<li><code class="docutils literal"><span class="pre">http://localhost:50070/</span></code></li>
<li><code class="docutils literal"><span class="pre">http://localhost:50030/</span></code></li>
</ul>
</div></blockquote>
<p>For cluster mode (<a class="reference internal" href="#setup-hadoop-3"><span>3b</span></a>), you'll use the name of the
master host in the URL and its port (see the web for details).</p>
</li>
</ul>
</li>
</ol>
<ol class="arabic" id="setup-hadoop-5" start="5">
<li><p class="first">Shut things down:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span>stop-all.sh   <span class="c1"># (This will stop hdfs and mapreduce)</span>
</pre></div>
</div>
</li>
</ol>
<ol class="arabic simple" id="setup-hadoop-6" start="6">
<li>Set up Chapel to run in distributed mode:<ul>
<li>You'll need to set up your Chapel environment to target multiple
locales in the standard way (see multilocale.rst and the
&quot;Settings to run Chapel on multiple nodes&quot; section of the
.bashrc below).</li>
<li>After this you should be able to run Chapel code with HDFS over
a cluster of computers the same way as you normally would.</li>
</ul>
</li>
</ol>
<p id="setup-hadoop-bashrc">Here is a sample .bashrc for using Hadoop within Chapel:</p>
<div class="highlight-sh"><div class="highlight"><pre><span></span><span class="c1">#</span>
<span class="c1"># For Hadoop</span>
<span class="c1">#</span>
<span class="nb">export</span> <span class="nv">HADOOP_INSTALL</span><span class="o">=</span>&lt;Place where you have Hadoop installed&gt;
<span class="nb">export</span> <span class="nv">HADOOP_HOME</span><span class="o">=</span><span class="nv">$HADOOP_INSTALL</span>
<span class="nb">export</span> <span class="nv">HADOOP_VERSION</span><span class="o">=</span>&lt;Your Hadoop version number&gt;
<span class="c1">#</span>
<span class="c1"># Note that the following environment variables might contain more paths than</span>
<span class="c1"># those listed below if you have more than one IO system enabled. These are all</span>
<span class="c1"># that you will need in order to use HDFS (only)</span>
<span class="c1">#</span>
<span class="nb">export</span> <span class="nv">CHPL_AUXIO_INCLUDE</span><span class="o">=</span><span class="s2">&quot;-I</span><span class="nv">$JAVA_INSTALL</span><span class="s2">/include -I</span><span class="nv">$JAVA_INSTALL</span><span class="s2">/include/linux  -I</span><span class="nv">$HADOOP_INSTALL</span><span class="s2">/src/c++/libhdfs&quot;</span>
<span class="nb">export</span> <span class="nv">CHPL_AUXIO_LIBS</span><span class="o">=</span><span class="s2">&quot;-L</span><span class="nv">$JAVA_INSTALL</span><span class="s2">/jre/lib/amd64/server -L</span><span class="nv">$HADOOP_INSTALL</span><span class="s2">/c++/Linux-amd64-64/lib&quot;</span>

<span class="c1">#</span>
<span class="c1"># So we can run things such as start-all.sh etc. from anywhere and</span>
<span class="c1"># don&#39;t need to be in $HADOOP_INSTALL</span>
<span class="c1">#</span>
<span class="nb">export</span> <span class="nv">PATH</span><span class="o">=</span><span class="nv">$PATH</span>:<span class="nv">$HADOOP_INSTALL</span>/bin

<span class="c1">#</span>
<span class="c1"># Point to the JDK installation</span>
<span class="c1">#</span>
<span class="nb">export</span> <span class="nv">JAVA_INSTALL</span><span class="o">=</span>&lt;Place where you have the jdk installed&gt;

<span class="c1">#</span>
<span class="c1"># Add Hadoop directories to the Java class path</span>
<span class="c1">#</span>
<span class="nb">export</span> <span class="nv">CLASSPATH</span><span class="o">=</span><span class="nv">$CLASSPATH</span>:<span class="nv">$HADOOP_HOME</span>/<span class="s2">&quot;&quot;</span>*:<span class="nv">$HADOOP_HOME</span>/lib/<span class="s2">&quot;&quot;</span>*:<span class="nv">$HADOOP_HOME</span>/conf/<span class="s2">&quot;&quot;</span>*:<span class="k">$(</span>hadoop classpath<span class="k">)</span>:

<span class="c1">#</span>
<span class="c1"># So we don&#39;t have to &quot;install&quot; these things</span>
<span class="c1">#</span>
<span class="nb">export</span> <span class="nv">LD_LIBRARY_PATH</span><span class="o">=</span><span class="nv">$LD_LIBRARY_PATH</span>:<span class="nv">$HADOOP_HOME</span>/c++/Linux-amd64-64/lib:<span class="nv">$HADOOP_HOME</span>/src/c++/libhdfs:<span class="nv">$JAVA_INSTALL</span>/jre/lib/amd64/server:<span class="nv">$JAVA_INSTALL</span>:<span class="nv">$HADOOP_HOME</span>/lib:<span class="nv">$JAVA_INSTALL</span>/jre/lib/amd64:<span class="nv">$CLASSPATH</span>

<span class="c1">#</span>
<span class="c1"># Settings to run Chapel on multiple nodes</span>
<span class="c1">#</span>
<span class="nb">export</span> <span class="nv">GASNET_SPAWNFN</span><span class="o">=</span>S
<span class="nb">export</span> <span class="nv">SSH_SERVERS</span><span class="o">=</span>&lt;the names of the computers in your cluster&gt;
<span class="nb">export</span> <span class="nv">SSH_CMD</span><span class="o">=</span>ssh
<span class="nb">export</span> <span class="nv">SSH_OPTIONS</span><span class="o">=</span>-x
<span class="nb">export</span> <span class="nv">GASNET_ROUTE_OUTPUT</span><span class="o">=</span>0
</pre></div>
</div>
</div>
<div class="section" id="hdfs-support-types-and-functions">
<h2>HDFS Support Types and Functions<a class="headerlink" href="#hdfs-support-types-and-functions" title="Permalink to this headline">¶</a></h2>
<dl class="record">
<dt id="HDFS.hdfsChapelFile">
<em class="property">record </em><code class="descname">hdfsChapelFile</code><a class="headerlink" href="#HDFS.hdfsChapelFile" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds a file per locale</p>
</dd></dl>

<dl class="record">
<dt id="HDFS.hdfsChapelFileSystem">
<em class="property">record </em><code class="descname">hdfsChapelFileSystem</code><a class="headerlink" href="#HDFS.hdfsChapelFileSystem" title="Permalink to this definition">¶</a></dt>
<dd><p>Holds a connection to HDFS per locale</p>
</dd></dl>

<dl class="function">
<dt id="HDFS.hdfsChapelConnect">
<em class="property">proc </em><code class="descname">hdfsChapelConnect</code><span class="sig-paren">(</span><em>out error: syserr</em>, <em>path: c_string</em>, <em>port: int</em><span class="sig-paren">)</span>: c_void_ptr<a class="headerlink" href="#HDFS.hdfsChapelConnect" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a connection to HDFS for a single locale</p>
</dd></dl>

<dl class="function">
<dt>
<em class="property">proc </em><code class="descname">hdfsChapelConnect</code><span class="sig-paren">(</span><em>path: string</em>, <em>port: int</em><span class="sig-paren">)</span>: hdfsChapelFileSystem</dt>
<dd><p>Connect to HDFS and create a filesystem ptr per locale</p>
</dd></dl>

<dl class="method">
<dt id="HDFS.hdfsChapelFileSystem.hdfsChapelDisconnect">
<em class="property">proc </em><code class="descclassname">hdfsChapelFileSystem.</code><code class="descname">hdfsChapelDisconnect</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#HDFS.hdfsChapelFileSystem.hdfsChapelDisconnect" title="Permalink to this definition">¶</a></dt>
<dd><p>Diconnect from the configured HDFS filesystem on each locale</p>
</dd></dl>

<dl class="method">
<dt id="HDFS.hdfsChapelFileSystem.hdfsOpen">
<em class="property">proc </em><code class="descclassname">hdfsChapelFileSystem.</code><code class="descname">hdfsOpen</code><span class="sig-paren">(</span><em>path: string</em>, <em>mode: iomode</em>, <em>hints: iohints = IOHINT_NONE</em>, <em>style: iostyle = defaultIOStyle()</em><span class="sig-paren">)</span>: hdfsChapelFile<a class="headerlink" href="#HDFS.hdfsChapelFileSystem.hdfsOpen" title="Permalink to this definition">¶</a></dt>
<dd><p>Open a file on each locale</p>
</dd></dl>

<dl class="method">
<dt id="HDFS.hdfsChapelFile.hdfsClose">
<em class="property">proc </em><code class="descclassname">hdfsChapelFile.</code><code class="descname">hdfsClose</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#HDFS.hdfsChapelFile.hdfsClose" title="Permalink to this definition">¶</a></dt>
<dd><p>Close a file opened with <a class="reference internal" href="#HDFS.hdfsChapelFileSystem.hdfsOpen" title="HDFS.hdfsChapelFileSystem.hdfsOpen"><code class="xref chpl chpl-proc docutils literal"><span class="pre">hdfsChapelFileSystem.hdfsOpen</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="HDFS.hdfsChapelFileSystem_local.hdfs_chapel_open">
<em class="property">proc </em><code class="descclassname">hdfsChapelFileSystem_local.</code><code class="descname">hdfs_chapel_open</code><span class="sig-paren">(</span><em>path: string</em>, <em>mode: iomode</em>, <em>hints: iohints = IOHINT_NONE</em>, <em>style: iostyle = defaultIOStyle()</em><span class="sig-paren">)</span>: file<a class="headerlink" href="#HDFS.hdfsChapelFileSystem_local.hdfs_chapel_open" title="Permalink to this definition">¶</a></dt>
<dd><p>Open a file on an HDFS filesystem for a single locale</p>
</dd></dl>

<dl class="function">
<dt id="HDFS.hdfs_chapel_connect">
<em class="property">proc </em><code class="descname">hdfs_chapel_connect</code><span class="sig-paren">(</span><em>path: string</em>, <em>port: int</em><span class="sig-paren">)</span>: hdfsChapelFileSystem_local<a class="headerlink" href="#HDFS.hdfs_chapel_connect" title="Permalink to this definition">¶</a></dt>
<dd><p>Connect to an HDFS filesystem on a single locale</p>
</dd></dl>

</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="HDFSiterator.html" class="btn btn-neutral float-right" title="HDFSiterator" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="FFTW_MT.html" class="btn btn-neutral" title="FFTW_MT" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Cray Inc.

    </p>
  </div> 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'1.13.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
  
 

</body>
</html>
