<?php
	$title="CHIUW 2023: 10th Annual Chapel Implementers and Users Workshop";
?>
<?php include("stdheader.html");?>

<p>&nbsp;</p>

<center>
<h1><big>CHIUW 2023</big></h1>
<h1><small>The 10th Annual</small><br>
Chapel Implementers and Users Workshop<br>

<h3>
  Coding Day<br>
  Thursday June 1st, 2023<br>
  Participants Only<br>
</h3>

<h3>
  Workshop Day<br>
  Friday June 2nd, 2023<br>
  8:00am&ndash;3:00pm PDT (GMT&ndash;7)<br>
  free and online via Zoom<br>
</h3>

</center>




<b><big>Summary</big></b><br>
<div class="indent">

<p>
CHIUW 2023 is the 10th annual <a href = "CHIUW.html">Chapel Implementers and
Users Workshop</a>, which serves as a forum where users and developers of the
general-purpose Chapel programming language (<a href =
"https://chapel-lang.org">chapel-lang.org</a>) can meet to report on work being
done with Chapel, exchange ideas, and forge new collaborations. Anyone
interested in parallel programming and/or Chapel is encouraged to attend CHIUW,
from long-term enthusiasts to those simply curious to learn more. This year's
CHIUW will be online and there will be no registration fees. 

</p>
</div>

<b><big>Registration</big></b><br>
<div class="indent">
<p>
Registration for CHIUW 2023 is free and can be completed at <a href =
  "https://hpe.zoom.us/meeting/register/tJUvcO6vrzsuG9AgFoSza-6xj_nQHQXtXSty">this
  link</a>.
</p>

<table summary = "times and events for CHIUW 2023">

  <tr>
    <td></td><td><center><big><u><b>Program</b></u></big></center></td>
  </tr>

  <tr>
    <td></td><td><center><b>Pre-Workshop</b></center></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>anytime&nbsp;</small></td> 
    <td><small><a id = "Chapel101"></a><b>Chapel 101</b> [<a href = "CHIUW/2020/CHIUW2020-Chapel101.pdf">slides</a> | <a href = "https://www.youtube.com/watch?v=w9AJZuCJ090">video</a>]</small></td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Brad Chamberlain</u> (<i>Hewlett Packard Enterprise</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td><td><small>This is a completely optional talk for those
    who are new to Chapel and looking for a crash-course, or for those
    who would simply appreciate a refresher.</small></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>



  <tr>
    <td></td><td><center><b>Thursday, June 1st</b></center></td>
  </tr>

  <tr>
    <td valign="top"><small>all day&nbsp;</small></td> 
    <td><small><b>Coding Day</b></small></td>
  </tr>
  <tr>
    <td></td>
    <td><small>
      This day will consist of asynchronous sessions where Chapel developers
      help users and enthusiasts with their Chapel code. Coding Day submissions
      are closed.
    </small></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td><td><center><b>Friday, June 2nd</b></center></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td><small><u><b>Time</b> (PDT)</u></small></td><td></td>
  </tr>


  <tr>
    <td valign="top"><small>8:00&ndash;8:35&nbsp;</small></td>
    <td>
      <small>
        <b>Welcome</b>
          [<a href = "CHIUW/2023/WelcomeSlides.pdf">slides</a>
           | <a href = "https://youtu.be/ttmy3dkeCgE">video</a>]
        <br>
        <b>State of the Project</b> 
          [<a href = "CHIUW/2023/Chamberlain-SoP.pdf">slides</a>
           | <a href = "https://youtu.be/QztOY1ezpo0">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Engin Kayraklioglu</u>, <u>Brad Chamberlain</u> (<i>Hewlett Packard
          Enterprise</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        This session will serve as a welcome to and overview of CHIUW 2023,
        along with a brief summary of highlights and milestones achieved within
        the Chapel project since last year.
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td>
    <td>
      <center> 
        <b>Session 1: Productivity</b><br>
        <small>Session chair: Daniel Fedorin (<i>HPE</i>)</small>
      </center>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>8:35&ndash;8:55&nbsp;</small></td>
    <td>
      <small>
        <b>Coupling Chapel-Powered HPC Workflows for Python</b>
        [<a href = "CHIUW/2023/Byrne.pdf">submission</a>
         | <a href = "CHIUW/2023/ByrneSlides.pdf">slides</a>
         | <a href = "https://youtu.be/0w-rJQO1xLs">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        John Byrne, <u>Harumi Kuno</u>, Chinmay Ghosh, Porno Shome, Amitha C, Sharad
        Singhal, Clarete Crasta, David Emberson, and Abhishek Dwaraki
        (<i>Hewlett Packard Enterprise</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b> 
          <font color="DarkBlue">
            Decades ago, when data analytics was known as data mining, there was
            an adage – “No data, no mining!” The pendulum has swung to the
            opposite extreme, as everything from hospitals to cars now produce
            massive quantities of data. We address the challenge of how to lower
            the barrier for efficiently processing massive quantities of data.
            We describe a solution that enables ordinary Python programmers to
            share results while working efficiently with datasets that can be
            too large to process using a single commodity machine. Our solution
            leverages Chapel, Arkouda, and OpenFAM to hide complexity,
            transparently enabling programmers to process large amounts of data
            on clusters of compute nodes while making it easy for them to share
            and incrementally maintain derived datasets.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>8:55&ndash;9:15&nbsp;</small></td>
    <td>
      <small>
        <b>Towards a Scalable Load Balancing for Productivity-Aware Tree-Search</b>
        [<a href = "CHIUW/2023/Helbecque.pdf">submission</a>
         | <a href = "CHIUW/2023/HelbecqueSlides.pdf">slides</a>
         | <a href = "https://youtu.be/vHIrKIROoYs">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Guillaume Helbecque</u>, Jan Gmys, Tiago Carneiro, Nouredine Melab and
        Pascal Bouvry (<i>Université de Lille</i>, <i>University of Luxembourg</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            In the context of exascale programming, we investigate a parallel
            distributed productivity-aware tree-search for exact optimization in
            Chapel. To this end, we present the DistBag-DFS distributed data
            structure, which is our revisited version of the Chapel’s DistBag
            data structure for depth-first search. The latter implements a
            distributed multi-pool, as well as an underlying locality-aware load
            balancing mechanism. Extensive experiments on large unbalanced
            tree-based problems are performed, and the competitiveness of our
            approach is reported against MPI+X implementations. For our best
            results, we achieve 94% of the ideal speed-up, using up to 64
            computer nodes (8192 cores).
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>


  <tr>
    <td></td><td><center><b>Break</b></center></td>
  </tr>

  <tr>
    <td valign="top"><small>9:15&ndash;9:30&nbsp;</small></td> 
    <td><small><b>Break</b></small></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td>
    <td>
      <center> 
        <b>Session 2: Optimizations and Portability</b><br>
        <small>Session chair: Dan Bonachea (<i>Lawrence Berkeley National
               Laboratory</i>)</small>
      </center>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>


  <tr>
    <td valign="top"><small>9:30&ndash;9:45&nbsp;</small></td>
    <td>
      <small>
        <b>High-Performance Programming and Execution of a Coral Biodiversity Mapping Algorithm Using Chapel</b>
        [<a href = "CHIUW/2023/Bachman.pdf">submission</a>
         | <a href = "CHIUW/2023/BachmanSlides.pdf">slides</a>
         | <a href = "https://youtu.be/lJhh9KLL2X0">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Scott Bachman</u>, Rebecca Green, Anna Bakker, Helen Fox, Sam Purkis
        and Ben Harshbarger (<i>National Center for Atmospheric Research</i>,
        <i>The Coral Reef Alliance</i>, <i>University of Miami</i>, <i>Hewlett
        Packard Enterprise</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            This paper will demonstrate how the parallelism and expressiveness
            of the Chapel programming language are used to achieve an enormous
            improvement in computational speed for a problem related to coral
            reef conservation. Chapel’s concise syntax and versatile data
            structures enable this problem to be solved in under 300 lines of
            code, while reducing the time to solution from days down to the
            order of seconds. This improvement is so substantial that it
            represents a paradigm shift in the way biodiversity can be measured
            at scale, providing a wealth of novel information for marine
            ecosystem managers and opening up brand new avenues for scientific
            inquiry. This paper will review the solution strategy and data
            structures in Chapel that allowed these improvements to be realized,
            and will preview future extensions of this work that have been made
            possible by this drastic speedup.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>9:45&ndash;10:00&nbsp;</small></td>
    <td>
      <small>
        <b>A Record-Based Pointer to Fabric Attached Memory</b>
        [<a href = "CHIUW/2023/C.pdf">submission</a>
         | <a href = "CHIUW/2023/CSlides.pdf">slides</a>
         | <a href = "https://youtu.be/c3KTaEwQzd0">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Amitha C</u>, Clarete Crasta, Brad Chamberlain, Sharad Singhal, Porno
        Shome and Dave Emberson (<i>Hewlett Packard Enterprise</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            Fabric Attached Memory (FAM) enables fast access to large datasets
            required in High Performance Data Analytics (HPDA) and Exploratory
            Data Analytics (EDA) applications. The Chapel language is
            designed for such applications and helps programmers via high-level
            programming constructs that are easy to use, while delegating the
            task of managing data and compute partitioning across the cluster to
            the Chapel compiler and runtime. Our previous work integrates
            FAM access within Chapel using a language-provided feature called
            user-defined array distributions. To support more general
            computational patterns using FAM from Chapel through abstracted
            language constructs, we have enabled a record-based pointer type to
            the FAM-resident data object and enabled access to the FAM memory
            through these pointers.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>10:00&ndash;10:15&nbsp;</small></td>
    <td>
      <small>
        <b>Automatic Adaptive Prefetching for Fine-Grain Communication in Chapel</b>
        [<a href = "CHIUW/2023/Rolinger.pdf">submission</a>
         | <a href = "CHIUW/2023/RolingerSlides.pdf">slides</a>
         | <a href = "https://youtu.be/TGl_2cFfD3o">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Thomas Rolinger</u> and Alan Sussman (<i>University of Maryland</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            Applications that operate on large, sparse graphs and matrices
            exhibit fine-grain irregular memory accesses patterns, leading to
            both performance and productivity challenges on today's
            distributed-memory systems. The Partitioned Global Address Space
            (PGAS) model attempts to address these challenges by combining the
            memory of physically distributed nodes into a logical global address
            space, simplifying how programmers perform communication in their
            applications. Chapel is an example of a programming language that
            implements a PGAS. However, while Chapel and the PGAS model can
            provide high developer productivity, the performance issues that
            arise from irregular memory accesses are still present. In this
            talk, we will discuss an approach to improve the performance of
            Chapel programs that exhibit fine-grain remote accesses while
            maintaining the high productivity benefits of the PGAS model. To
            achieve this goal, we designed and implemented a compiler
            optimization that performs adaptive prefetching for remote data.
            Specifically, the compiler performs static analysis to identify
            irregular memory access patterns to distributed arrays in parallel
            loops and then applies code transformations to prefetch remote data
            that will be needed in future loop iterations. Our approach is
            adaptive because the prefetch distance (i.e., how many iterations
            ahead to prefetch) is automatically adjusted as the program executes
            to ensure the prefetches are not issued too early or too late.
            Furthermore, the optimization is fully automatic and requires no
            user intervention. We demonstrate runtime speed-ups as large as 3.2x
            via adaptive prefetching when compared to unoptimized baseline
            implementations of various irregular workloads across three
            different distributed-memory systems.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>



  <tr>
    <td></td><td><center><b>Break</b></center></td>
  </tr>

  <tr>
    <td valign="top"><small>10:15&ndash;10:30&nbsp;</small></td> 
    <td><small><b>Break</b></small></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td>
    <td>
      <center> 
        <b>Keynote</b><br>
        <small>Session chair: Brad Chamberlain (<i>HPE</i>)</small>
      </center>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>


  <tr>
    <td valign="top"><small>10:30&ndash;11:30&nbsp;</small></td>
    <td>
      <small><a id = "keynote"></a>
        <b>PGAS Programming Models: My 20-year Perspective</b>
        [<a href = "CHIUW/2023/HargroveSlides.pdf">slides</a>
         | <a href = "https://youtu.be/AJVW1pG6zPo">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Paul Hargrove</u> (<i>Lawrence Berkeley National Laboratory</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <table>
      <tr>
      <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            Paul H. Hargrove has been involved in the world of Partitioned
            Global Address Space (PGAS) programming models since 1999, before he
            knew such a thing existed.  Early involvement in the GASNet
            communications library as used in implementations of UPC, Titanium
            and Co-array Fortran convinced Paul that one could have productivity
            and performance without sacrificing one for the other.  Since then
            he has been among the apostates who work to overturn the belief that
            message-passing is the only (or best) way to program for
            High-Performance Computing (HPC).  Paul has been fortunate to
            witness the history of the PGAS community through several rare
            opportunities, including interactions made possible by the wide
            adoption of GASNet and through operating a PGAS booth at the annual
            SC conferences from 2007 to 2017.  In this talk, Paul will share
            some highlights of his experiences across 24 years of PGAS history.
            Among these is the DARPA High Productivity Computing Systems (HPCS)
            project which helped give birth to Chapel. 
           </font>
      <small>
      </td>
      <td>
        <img src="images/hargrove.jpeg" alt="Smiley face" width="160"
        style="float:right">
      </td>
      </tr>
      </table>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Bio:</b>
          <font color="DarkGreen"> 
            Dr. Paul Hargrove received his Ph.D. from Stanford University's
            Program in Scientific Computing and Computational Mathematics in
            2004. Prior to that Paul received a Bachelor of Arts degree from
            Cornell University in 1994, completing a triple major in Physics
            (magna cum laude), Math, and Computer Science.  

            <p>
            Paul has been a PI at Lawrence Berkeley National Lab (LBNL) since
            September 2000, following periods of summer and part-time employment
            at LBNL. His current research focuses on network communications for
            HPC, with current software projects including UPC++ and Global
            Address Space Networking (GASNet-EX). Paul is PI of the Pagoda
            project, funded by the US Department of Energy's Exascale Computing
            Project (ECP), under which UPC++ and GASNet-EX are developed.
            </p>
         </font>
      </small>
    </td>
  </tr>


  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td><td><center><b>Break</b></center></td>
  </tr>

  <tr>
    <td valign="top"><small>11:30&ndash;11:45&nbsp;</small></td> 
    <td><small><b>Break</b></small></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td>
    <td>
      <center> 
        <b>Session 3: Arkouda</b><br>
        <small>Session chair: Josh Milthorpe (<i>Oak Ridge National Laboratory</i>)</small>
      </center>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>11:45&ndash;12:00&nbsp;</small></td>
    <td>
      <small>
        <b>Too Big to Fail: Massive Scale Linear Algebra with Chapel and Arkouda</b>
        [<a href = "CHIUW/2023/Hollis.pdf">submission</a>
         | <a href = "CHIUW/2023/HollisSlides.pdf">slides</a>
         | <a href = "https://youtu.be/1rTaV8BMltc">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Christopher Hollis</u> (<i>U.S. Department of Defense</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            This presentation details the development of a linear algebra
            extension for Arkouda (a NumPy-like Python application that
            utilizes Chapel for a backend server). This interface, dubbed
            AkSparse, allows for the creation and manipulation of sparse
            matrices at large scale with features designed to be familiar to
            users of SciPy’s existing sparse array package. This includes a
            sparse general matrix-matrix multiplication (SpGEMM) implemented
            with a novel algorithm leveraging the strengths of both Arkouda and
            Chapel. AkSparse allows users to integrate linear algebraic
            techniques into existing exploratory data analysis (EDA) workflows
            on datasets at a scale not previously possible.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>12:00&ndash;12:15&nbsp;</small></td>
    <td>
      <small>
        <b>Minimum-Mapping based Connected Components Algorithm</b>
        [<a href = "CHIUW/2023/Du.pdf">submission</a>
         | <a href = "CHIUW/2023/DuSlides.pdf">slides</a>
         | <a href = "https://youtu.be/SW2JVG9Z4ng">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Zhihui Du</u>, Oliver Alvarado Rodriguez, Fuhuan Li, Mohammad
        Dindoost and David A. Bader (<i>New Jersey Institute of Technology</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            Finding connected components is a fundamental problem in graph
            analysis. We develop a novel minimum-mapping based Contour algorithm
            to solve the connectivity problem. The Contour algorithm can
            identify all connected components of an undirected graph within
            O(log(dmax)) iterations on m parallel processors, where dmax is the
            largest diameter of all components in a given graph and $m$ is the
            total number of edges of the given graph. Furthermore, each
            iteration can easily be parallelized by employing the highly
            efficient minimum-mapping operator on all edges. To improve
            performance, the Contour algorithm is further optimized through
            asynchronous updates and simplified atomic operations. Our algorithm
            has been integrated into an open-source framework, Arachne, that
            extends Arkouda for large-scale interactive graph analytics with a
            Python API powered by the high-productivity parallel language
            Chapel. Experimental results on real-world and synthetic graphs show
            that the proposed Contour algorithm needs less number of iterations
            and can achieve 5.26 folds of speedup on average compared with the
            state-of-the-art connected component method FastSV implemented in
            Chapel. All code is publicly available on GitHub
            (https://github.com/Bears-R-Us/arkouda-njit).
          </font>
      </small>
    </td>
  </tr>


  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>12:15&ndash;12:30&nbsp;</small></td>
    <td>
      <small>
        <b>Removing Temporary Arrays in Arkouda</b>
        [<a href = "CHIUW/2023/McDonald.pdf">submission</a>
         | <a href = "CHIUW/2023/McDonaldSlides.pdf">slides</a>
         | <a href = "https://youtu.be/3j95XtBvhTY">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Ben McDonald</u> (<i>Hewlett Packard Enterprise</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            This talk discusses experimental modifications made to the Arkouda
            (a NumPy-like Python package with a Chapel backend server) messaging
            layer to pass several operations together as a block of Lisp code to
            be parsed on the server in one message, as opposed to the existing
            model of each command being passed as individual messages, requiring
            multiple passes to evaluate compound expressions. These
            modifications were made to eliminate the need for extra temporary
            array creation when executing compound operations in Arkouda. To
            improve the performance of the implementation, the initial code,
            which parsed the Lisp code once per-task, was optimized to parse
            only once per message and remove dynamic allocations. The
            implementation is evaluated by comparing it against current Arkouda
            performance. The results of the comparison show that the Lisp
            interpreter is not yet outperforming standard Arkouda code, but
            additional functionality can be supported through this new feature.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>


  <tr>
    <td></td><td><center><b>Break</b></center></td>
  </tr>

  <tr>
    <td valign="top"><small>12:30&ndash;12:45&nbsp;</small></td> 
    <td><small><b>Break</b></small></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td>
    <td>
      <center> 
        <b>Session 4: Applications and Performance Analysis</b><br>
        <small>Session chair: Ben McDonald (<i>HPE</i>)</small>
      </center>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>12:45&ndash;12:55&nbsp;</small></td>
    <td>
      <small>
        <b>Development of a Knowledge-Sharing Parallel Computing Approach for Calibrating Distributed Watershed Hydrologic Models</b>
        [<a href = "CHIUW/2023/Asgari.pdf">submission</a>
         | <a href = "CHIUW/2023/AsgariSlides.pdf">slides</a>
         | <a href = "https://youtu.be/eN6hAQg0uko">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Marjan Asgari</u> (<i>University of Guelph</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            A research gap in calibrating distributed watershed hydrologic
            models lies in the development of calibration frameworks adaptable
            to increasing complexity of hydrologic models. Parallel computing is
            a promising approach to address this gap. However, parallel
            calibration approaches should be fault-tolerant, portable, and easy
            to implement with minimum communication overhead for fast knowledge
            sharing between parallel nodes. Accordingly, we developed a
            knowledge-sharing parallel calibration approach using Chapel
            programming language, with which we implemented the Parallel
            Dynamically Dimensioned Search (DDS) algorithm by adopting multiple
            perturbation factors and parallel dynamic searching strategies to
            keep a balance between exploration and exploitation of the search
            space. Our results showed that this approach achieved super-linear
            speedup and parallel efficiency above 75%. In addition, our approach
            has a low communication overhead, along with the positive impact of
            knowledge-sharing in the convergence behavior of the parallel DDS
            algorithm.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>12:55&ndash;1:05&nbsp;</small></td>
    <td>
      <small>
        <b>Parallel Implementation in Chapel for the Numerical Solution of the 3D Poisson Problem</b>
        [<a href = "CHIUW/2023/Jesus.pdf">submission</a>
         | <a href = "CHIUW/2023/JesusSlides.pdf">slides</a>
         | <a href = "https://youtu.be/fKp-lSpG9aE">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Anna Jesus</u>, Livia Freire, Willian Carlos Lesinhovski and Nelson
        Dias (<i>University of Sao Paulo</i>, <i>Federal University of
        Paraná</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            In this study, we present a parallel implementation of the numerical
            Poisson equation with domain decomposition in three directions using
            the Chapel programming language. Our goal is to study the potential
            of Chapel as an easy-to-implement alternative to a code originally
            developed in Fortran+MPI. The numerical experiments were performed
            on the cluster of the Instituto de Ciências Matemáticas e de
            Computação of the University of São Paulo, on a grid 1303 points,
            corresponding to 2097152 unknowns. The results, for a single node
            only, suggest that the performance of Chapel tends to vary between
            30-80% compared to the Fortran+MPI code with up to 32 threads.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>1:05&ndash;1:20&nbsp;</small></td>
    <td>
      <small>
        <b>Runtime Comparison Between Chapel and Fortran</b>
        [<a href = "CHIUW/2023/Lesinhovski.pdf">submission</a>
         | <a href = "CHIUW/2023/LesinhovskiSlides.pdf">slides</a>
         | <a href = "https://youtu.be/VQSYhhjuXYI">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Willian Lesinhovski</u>, Nelson Dias, Livia Freire and Anna Jesus
        (<i>Federal University of Paraná</i>, <i>University of Sao Paulo</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            In this text we present a simple but interesting runtime comparison
            between Chapel and Fortran when performing some very common
            algorithms in numerical analysis: matrix multiplication, Lax method
            for the kinematic wave equation and SOR method for the Poisson
            equation. Chapel presented a very satisfactory performance reducing
            the processing time from 10% to 50% compared to Fortran.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>


  <tr>
    <td></td><td><center><b>Break</b></center></td>
  </tr>

  <tr>
    <td valign="top"><small>1:20&ndash;1:35&nbsp;</small></td> 
    <td><small><b>Break</b></small></td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td></td>
    <td>
      <center> 
        <b>Session 5: GPUs</b><br>
        <small>Session chair: Harumi Kuno (<i>HPE</i>)</small>
      </center>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>1:35&ndash;1:50&nbsp;</small></td>
    <td>
      <small>
        <b>Accelerating Data Analytics with Arkouda on GPUs</b>
        [<a href = "CHIUW/2023/Milthorpe.pdf">submission</a>
         | <a href = "CHIUW/2023/MilthorpeSlides.pdf">slides</a>
         | <a href = "https://youtu.be/mGlusYfK5DE">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Josh Milthorpe</u>, Brett Eiffert and Jeffrey Vetter (<i>Oak Ridge
          National Laboratory</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            In this talk, we will use demonstrate how the Chapel GPU API can be
            used to accelerate Arkouda operations, which is most beneficial when
            a chain of operations is executed on the same data. We extend the
            GPU API to support shared virtual memory using CUDA unified memory
            and use this support to implement a custom domain map for Arkouda
            arrays. Our preliminary performance results show that
            GPU-accelerated operations in unified memory perform comparably or
            better than explicit memory management while simplifying the
            programming task for complex Arkouda operations.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>1:50&ndash;2:05&nbsp;</small></td>
    <td>
      <small>
        <b>Enabling CHIP-SPV in Chapel GPUAPI module</b>
        [<a href = "CHIUW/2023/Zhao.pdf">submission</a>
         | <a href = "CHIUW/2023/ZhaoSlides.pdf">slides</a>
         | <a href = "https://youtu.be/Dt_ds6yXXV8">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        Jisheng Zhao, <u>Akihiro Hayashi</u>, Brice Videau and Vivek Sarkar
        (<i>Georgia Institute of Technology</i>, <i>Argonne National
        Laboratory</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            This talk discusses enhancing support for Intel GPUs in the Chapel
            GPUAPI module. Essentially, we introduce the CHIP-SPV framework as a
            backend for the module, allowing the user to run their hand-written
            CUDA/HIP kernels on Intel GPUs as-is from their Chapel programs and
            allowing the runtime to perform finer-grain control of Intel GPUs
            through Intel Level Zero runtime. In particular, we discuss the
            design and implementation of our CHIP-SPV backend in the GPUAPI
            module and demonstrate a preliminary performance evaluation of the
            backend on an Intel GPU platform. We also plan to discuss the
            possibility of using CHIP- SPV as a general code generation target
            in Chapel’s GPU code generator to enhance its portability.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>2:05&ndash;2:20&nbsp;</small></td>
    <td>
      <small>
        <b>Initial Experiences in Porting a GPU Graph Analysis Workload from CUDA/SYCL to Chapel</b>
        [<a href = "CHIUW/2023/Sathre.pdf">submission</a>
         | <a href = "CHIUW/2023/SathreSlides.pdf">slides</a>
         | <a href = "https://youtu.be/dszarKaDoVU">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Paul Sathre</u>, Atharva Gondhalekar and Wu-Chun Feng (<i>Virginia
          Tech</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
            In this talk, we will discuss our initial experiences in porting a
            GPU graph analysis proxy workload from CUDA/SYCL to Chapel. This
            endeavor is part of a broader study to characterize the
            performance-productivity tradeoffs of Chapel's new native support
            for compiling loops for GPU execution. We will discuss the
            motivation for migrating to Chapel, provide an introduction to our
            proxy application known as edge-connected Jaccard similarity, and
            briefly discuss code migration issues and preliminary performance
            observations.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>2:20&ndash;2:40&nbsp;</small></td>
    <td>
      <small>
        <b>Recent GPU Programming Improvements in Chapel</b>
        [<a href = "CHIUW/2023/Kayraklioglu.pdf">submission</a>
         | <a href = "CHIUW/2023/KayrakliogluSlides.pdf">slides</a>
         | <a href = "https://youtu.be/vDTyVTbtuD0">video</a>]
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <u>Engin Kayraklioglu</u>, Andy Stone and Daniel Fedorin (<i>Hewlett
          Packard Enterprise</i>)
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        <b>Abstract:</b>
          <font color="DarkBlue">
              Chapel’s emerging native GPU programming support has improved
              considerably in the last year. In this talk, we will highlight
              some of the improvements and discuss our next steps.
          </font>
      </small>
    </td>
  </tr>

  <tr>
    <td>&nbsp;</td>
  </tr>
  <tr>
    <td>&nbsp;</td>
  </tr>

  <tr>
    <td valign="top"><small>2:40&ndash;?:??&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</small></td>
    <td>
      <small>
        <b>Open Discussion Session</b>
      </small>
    </td>
  </tr>
  <tr>
    <td></td>
    <td>
      <small>
        This final session is designed to support open discussion and
        interaction among the CHIUW attendees, and to provide an
        opportunity for lightning talks.  This year's session
        included:

        <ul>

        <li> Tom Westerhout giving a lightning talk on a recent
             project he's been working on
          <a href = "https://github.com/twesterhout/nix-chapel">to
             combine the Nix package manager/build system with
             Chapel</a>

<p>

        <li> S. Isaac Geronimo Anderson giving a lightning talk on a
             pre-print of his paper, <a href =
             "CHIUW/2023/Anderson.pdf"><i>Computing Sparse Tensor
             Decompositions via Chapel and C++/MPI Interoperability
             without Intermediate I/O</i></a> [<a href =
             "https://youtu.be/y4BTlNrprqA">video</a>]

<p>

        <li> An impromptu discussion on IDE support for Chapel, a
             verbal preview of Chapel support for the Language Server
             Protocol being developed through Dyno, and wishes for
             other tools.

<p>

        <li> Reminiscences and musings about the cross-language forums
             that existed during the DARPA HPCS program, what it would
             take for such things to exist today, and what's changed
             over that time period.

        </ul>

      </small>
    </td>
  </tr>


  <tr>
    <td>&nbsp;</td>
  </tr>
</table>


<p>
<a id = "organization"></a>
<b><big>Organization</big></b><br>
</p>
<div class="indent">
<p>
<b>General Chair:</b>
<ul>
<li> Michelle Strout, <i>HPE</i><br>
</ul>

<b>Program Committee:</b>
<ul>
<li> Engin Kayraklioglu (chair), <i>HPE</i><br>
<li> Dave Wonnacott (co-chair), <i>Haverford College</i><br>
<li> Scott Bachman, <i>National Center for Atmospheric Research/HPE</i><br>
<li> Dan Bonachea, <i>Lawrence Berkeley National Laboratory</i><br>
<li> Maryam Mehri Dehnavi, <i>University of Toronto</i><br>
<li> Nelson Luís Dias, <i>Federal University of Paraná</i><br>
<li> Akihiro Hayashi, <i>Georgia Tech</i><br>
<li> Harumi Kuno, <i>HPE</i><br>
<li> Josh Milthorpe, <i>Oak Ridge National Laboratory</i><br>
<li> Thomas Rolinger, <i>University of Maryland</i><br>
<li> Rich Vuduc, <i>Georgia Tech</i><br>
<li> Andrew Younge, <i>Sandia National Laboratories</i><br>

</ul>


<b>Steering Committee:</b>
<ul>
<li> Brad Chamberlain, <i>HPE</i><br>
<li> Éric Laurendeau, <i>Polytechnique Montreal</i><br>
<li> Bill Reus, <i>US DoD</i><br>
<li> Didem Unat, <i>Koc University</i><br>
</ul>
</p>


</div>

<p>&nbsp;</p>

<p>
<small><a href = "CHIUW2023-cfp.html"><b>Call For Papers and Talks</b></a> (for
archival purposes)</small>
</p>

<?php include("stdfooter.html");?>
